server:
  port: 50004

spring:
  application:
    name: llm-analyzer-service
  cloud:
    compatibility-verifier:
      enabled: false
  
  # Redis 설정 (캐시용)
  data:
    redis:
      host: localhost
      port: 6380
      password:
      database: 0
      timeout: 2000ms
      jedis:
        pool:
          max-active: 8
          max-wait: -1ms
          max-idle: 8
          min-idle: 0

  # WebFlux 설정 (base-path 제거 - 컨트롤러에서 /api 경로 직접 관리)

  # Jackson 설정
  jackson:
    property-naming-strategy: SNAKE_CASE
    default-property-inclusion: NON_NULL

# Eureka 클라이언트 설정
eureka:
  client:
    service-url:
      defaultZone: http://eureka:eureka_secret@localhost:50008/eureka/
    fetch-registry: true
    register-with-eureka: true
    instance-info-replication-interval-seconds: 10
    registry-fetch-interval-seconds: 10
  instance:
    hostname: localhost
    prefer-ip-address: true
    lease-renewal-interval-in-seconds: 5
    lease-expiration-duration-in-seconds: 10
    metadata-map:
      zone: local
      profile: dev

# Circuit Breaker 설정 (LLM API 호출용)
resilience4j:
  circuitbreaker:
    instances:
      gemini-api:
        register-health-indicator: true
        sliding-window-size: 10
        minimum-number-of-calls: 5
        permitted-number-of-calls-in-half-open-state: 3
        wait-duration-in-open-state: 30s
        failure-rate-threshold: 50
        slow-call-rate-threshold: 50
        slow-call-duration-threshold: 10s
      openai-api:
        register-health-indicator: true
        sliding-window-size: 10
        minimum-number-of-calls: 5
        permitted-number-of-calls-in-half-open-state: 3
        wait-duration-in-open-state: 30s
        failure-rate-threshold: 50
        slow-call-rate-threshold: 50
        slow-call-duration-threshold: 15s
  retry:
    instances:
      llm-api:
        max-attempts: 3
        wait-duration: 1s
        retry-exceptions:
          - java.net.ConnectException
          - java.net.SocketTimeoutException
  timelimiter:
    instances:
      llm-api:
        timeout-duration: 60s

# 관찰성 설정
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
      base-path: /actuator
  endpoint:
    health:
      show-details: always
    metrics:
      enabled: true
  metrics:
    export:
      prometheus:
        enabled: true
  tracing:
    sampling:
      probability: 1.0  # Netflix/Google 수준: 모든 요청 추적
  zipkin:
    tracing:
      endpoint: http://localhost:59411/api/v2/spans

# 로깅 설정
logging:
  level:
    root: INFO
    com.holo.oshi: DEBUG
    org.springframework.web: DEBUG
    io.netty: INFO
  pattern:
    console: "%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
  file:
    name: logs/llm-analyzer-service.log

# AI Provider 설정
ai:
  providers:
    google:
      api-key: ${GOOGLE_API_KEY:}
      project-id: ${GOOGLE_PROJECT_ID:holo-oshi-finder}
      location: us-central1
      models:
        primary: gemini-2.5-pro
        fallback: gemini-2.5-flash
        embedding: gemini-embedding-001
    openai:
      api-key: ${OPENAI_API_KEY:}
      models:
        primary: gpt-4o
        fallback: gpt-4o-mini
        embedding: text-embedding-3-large
  
  # 분석 설정
  analysis:
    max-candidates: 10
    confidence-threshold: 0.6
    cache-duration: 2h
    timeout-seconds: 60
    
    # 토큰 제한
    tokens:
      max-prompt: 32000
      max-completion: 8192
      
  # 폴백 설정
  fallback:
    enabled: true
    max-retries: 2
    strategy: cross-provider  # same-provider, cross-provider

# 캐시 설정
cache:
  redis:
    enabled: true
    default-ttl: 7200  # 2시간
    prefixes:
      survey-analysis: "llm:survey:"
      final-analysis: "llm:final:"
      model-response: "llm:response:"